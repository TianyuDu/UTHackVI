{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import _pickle as pickle\n",
    "from typing import List\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import chakin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download embedding? >>> \n",
      "                   Name  Dimension                     Corpus VocabularySize  \\\n",
      "2          fastText(en)        300                  Wikipedia           2.5M   \n",
      "11         GloVe.6B.50d         50  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "12        GloVe.6B.100d        100  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "13        GloVe.6B.200d        200  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "14        GloVe.6B.300d        300  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "15       GloVe.42B.300d        300          Common Crawl(42B)           1.9M   \n",
      "16      GloVe.840B.300d        300         Common Crawl(840B)           2.2M   \n",
      "17    GloVe.Twitter.25d         25               Twitter(27B)           1.2M   \n",
      "18    GloVe.Twitter.50d         50               Twitter(27B)           1.2M   \n",
      "19   GloVe.Twitter.100d        100               Twitter(27B)           1.2M   \n",
      "20   GloVe.Twitter.200d        200               Twitter(27B)           1.2M   \n",
      "21  word2vec.GoogleNews        300          Google News(100B)           3.0M   \n",
      "\n",
      "      Method Language    Author  \n",
      "2   fastText  English  Facebook  \n",
      "11     GloVe  English  Stanford  \n",
      "12     GloVe  English  Stanford  \n",
      "13     GloVe  English  Stanford  \n",
      "14     GloVe  English  Stanford  \n",
      "15     GloVe  English  Stanford  \n",
      "16     GloVe  English  Stanford  \n",
      "17     GloVe  English  Stanford  \n",
      "18     GloVe  English  Stanford  \n",
      "19     GloVe  English  Stanford  \n",
      "20     GloVe  English  Stanford  \n",
      "21  word2vec  English    Google  \n"
     ]
    }
   ],
   "source": [
    "DOWNLOAD = bool(input(\"Download embedding? >>> \").upper() == \"Y\")\n",
    "chakin.search(lang=\"English\")\n",
    "if DOWNLOAD:\n",
    "    chakin.download(number=11, save_dir=\"../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_import import load_embedding_from_disks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter\n",
    "# GLOVE_FILENAME = \"../data/glove.840B.300d.txt\"\n",
    "GLOVE_FILENAME = \"../data/glove.6B.50d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['enthusiasm', 'worry', 'love', 'empty', 'hate', 'sadness', 'boredom', 'anger', 'neutral', 'fun', 'surprise', 'relief', 'happiness']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./text_emotion.csv\")\n",
    "df.head()\n",
    "SENTI_LST = list(set(df[\"sentiment\"]))\n",
    "print(SENTI_LST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5253it [00:00, 52529.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding from disks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:07, 51668.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding loaded from disks.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading embedding from disks...\")\n",
    "word_to_index, index_to_embedding = load_embedding_from_disks(GLOVE_FILENAME, with_indexes=True)\n",
    "print(\"Embedding loaded from disks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 400001\n",
      "Embedding Dim: 50\n"
     ]
    }
   ],
   "source": [
    "vocab_size, embedding_dim = index_to_embedding.shape\n",
    "print(f\"Vocab Size: {vocab_size}\\nEmbedding Dim: {embedding_dim}\")\n",
    "\n",
    "lstm_units = (512, 1024)\n",
    "num_classes = 13\n",
    "iterations = 50\n",
    "max_seq_length = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2int(w: str) -> int:\n",
    "    try:\n",
    "        idx = word_to_index[w]\n",
    "    except KeyError:\n",
    "        idx = word_to_index[\"unk\"]\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 40)\n",
      "(40000, 13)\n"
     ]
    }
   ],
   "source": [
    "X_lst, y_lst = [], []\n",
    "for sentence, senti in zip(df[\"content\"], df[\"sentiment\"]):\n",
    "    # ==== Encode x ====\n",
    "    tokens = sentence.lower().split()\n",
    "    word_ints = np.array([word2int(x) for x in tokens])\n",
    "    X_lst.append(word_ints)\n",
    "    \n",
    "    # ==== Encode y ====\n",
    "    label = np.zeros([num_classes])\n",
    "    senti_index = SENTI_LST.index(senti)\n",
    "    label[senti_index] = 1\n",
    "    y_lst.append(label)\n",
    "    \n",
    "X_lst = pad_sequences(\n",
    "    X_lst,\n",
    "    maxlen=max_seq_length,\n",
    "    padding=\"post\",\n",
    "    truncating=\"post\"\n",
    ")\n",
    "\n",
    "X_raw = np.stack(X_lst)\n",
    "y_raw = np.stack(y_lst)\n",
    "print(X_raw.shape)\n",
    "print(y_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test,\n",
    " y_train, y_test) = train_test_split(\n",
    "    X_raw, y_raw,\n",
    "    test_size=0.2,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "(X_train, X_val,\n",
    " y_train, y_val) = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size=0.2,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing set generated,\n",
      "X_train shape: (25600, 40)\n",
      "y_train shape: (25600, 13)\n",
      "X_test shape: (8000, 40)\n",
      "y_test shape: (8000, 13)\n",
      "X_validation shape: (6400, 40)\n",
      "y_validation shape: (6400, 13)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training and testing set generated,\\\n",
    "\\nX_train shape: {X_train.shape}\\\n",
    "\\ny_train shape: {y_train.shape}\\\n",
    "\\nX_test shape: {X_test.shape}\\\n",
    "\\ny_test shape: {y_test.shape}\\\n",
    "\\nX_validation shape: {X_val.shape}\\\n",
    "\\ny_validation shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_batches = X_train.reshape(25, 1024, max_seq_length)\n",
    "y_train_batches = y_train.reshape(25, 1024, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sess.close()\n",
    "except NameError:\n",
    "    print(\"Session already cleaned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "with tf.name_scope(\"DATA_IO\"):\n",
    "    word_ids = tf.placeholder(\n",
    "        tf.int32,\n",
    "        shape=[None, max_seq_length]\n",
    "    )\n",
    "    \n",
    "    y = tf.placeholder(\n",
    "        tf.float32,\n",
    "        shape=[None, num_classes]\n",
    "    )\n",
    "\n",
    "with tf.name_scope(\"EMBEDDING\"):\n",
    "    embedding = tf.Variable(\n",
    "        tf.constant(0.0, shape=index_to_embedding.shape),\n",
    "        trainable=False,\n",
    "        name=\"EMBEDDING\"\n",
    "    )\n",
    "    \n",
    "    word_representation_layer = tf.nn.embedding_lookup(\n",
    "        params=embedding,\n",
    "        ids=word_ids\n",
    "    )\n",
    "    \n",
    "    embedding_placeholder = tf.placeholder(\n",
    "        tf.float32,\n",
    "        shape=index_to_embedding.shape\n",
    "    )\n",
    "    \n",
    "    embedding_init = embedding.assign(embedding_placeholder)\n",
    "    \n",
    "    _ = sess.run(\n",
    "        embedding_init, \n",
    "            feed_dict={\n",
    "                embedding_placeholder: index_to_embedding\n",
    "        }\n",
    "    )\n",
    "\n",
    "with tf.name_scope(\"RNN\"):\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell(\n",
    "        [tf.nn.rnn_cell.LSTMCell(\n",
    "            num_units=units,\n",
    "            name=f\"LSTM_LAYER_{i}\")\n",
    "            for i, units in enumerate(lstm_units)\n",
    "         ])\n",
    "    \n",
    "    lstm_cell = tf.contrib.rnn.DropoutWrapper(\n",
    "        cell=cell,\n",
    "        output_keep_prob=0.75\n",
    "    )\n",
    "    outputs, state = tf.nn.dynamic_rnn(\n",
    "        lstm_cell, \n",
    "        word_representation_layer,\n",
    "        dtype=tf.float32\n",
    "    )\n",
    "\n",
    "with tf.name_scope(\"OUTPUT\"):\n",
    "    weight = tf.Variable(\n",
    "        tf.truncated_normal(\n",
    "            [lstm_units[-1], num_classes]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    bias = tf.Variable(\n",
    "        tf.random_normal(shape=[num_classes])\n",
    "    )\n",
    "\n",
    "# Option i)\n",
    "    value = tf.transpose(outputs, [1, 0, 2])\n",
    "    last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "# Option ii)\n",
    "#     last = outputs[:, -1, :]\n",
    "    pred = tf.matmul(last, weight) + bias\n",
    "    pred_idx = tf.argmax(pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Accuracy:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.name_scope(\"METRICS\"):\n",
    "    correct_pred = tf.equal(\n",
    "        tf.argmax(pred, axis=1),\n",
    "        tf.argmax(y, axis=1)\n",
    "    )\n",
    "\n",
    "    accuracy = tf.reduce_mean(\n",
    "        tf.cast(correct_pred, tf.float32)\n",
    "    )\n",
    "\n",
    "with tf.name_scope(\"LOSSES\"):\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=pred,\n",
    "            labels=y\n",
    "        )\n",
    "    )\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "tf.summary.scalar(\"Loss\", loss)\n",
    "tf.summary.scalar(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs[0]: train batch avg accuracy=0.2049218714237213, val accuracy=0.21484375\n",
      "Epochs[1]: train batch avg accuracy=0.2126171886920929, val accuracy=0.21328124403953552\n",
      "Epochs[2]: train batch avg accuracy=0.21238280832767487, val accuracy=0.21796874701976776\n",
      "Epochs[3]: train batch avg accuracy=0.2116406261920929, val accuracy=0.21453124284744263\n",
      "Epochs[4]: train batch avg accuracy=0.19800780713558197, val accuracy=0.20031249523162842\n",
      "Epochs[5]: train batch avg accuracy=0.1947656273841858, val accuracy=0.20156249403953552\n",
      "Epochs[6]: train batch avg accuracy=0.20496094226837158, val accuracy=0.2082812488079071\n",
      "Epochs[7]: train batch avg accuracy=0.20929688215255737, val accuracy=0.20656250417232513\n",
      "Epochs[8]: train batch avg accuracy=0.20921875536441803, val accuracy=0.21312500536441803\n",
      "Epochs[9]: train batch avg accuracy=0.21355468034744263, val accuracy=0.21765625476837158\n",
      "Epochs[10]: train batch avg accuracy=0.21265624463558197, val accuracy=0.21546874940395355\n",
      "Epochs[11]: train batch avg accuracy=0.21363280713558197, val accuracy=0.21781249344348907\n",
      "Epochs[12]: train batch avg accuracy=0.21238280832767487, val accuracy=0.2150000035762787\n",
      "Epochs[13]: train batch avg accuracy=0.21355468034744263, val accuracy=0.2150000035762787\n",
      "Epochs[14]: train batch avg accuracy=0.21292968094348907, val accuracy=0.21671874821186066\n",
      "Epochs[15]: train batch avg accuracy=0.2138671875, val accuracy=0.21578125655651093\n",
      "Epochs[16]: train batch avg accuracy=0.21464844048023224, val accuracy=0.21781249344348907\n",
      "Epochs[17]: train batch avg accuracy=0.21355468034744263, val accuracy=0.22109374403953552\n",
      "Epochs[18]: train batch avg accuracy=0.21179687976837158, val accuracy=0.21203124523162842\n",
      "Epochs[19]: train batch avg accuracy=0.21054688096046448, val accuracy=0.21531249582767487\n",
      "Epochs[20]: train batch avg accuracy=0.20945312082767487, val accuracy=0.2109375\n",
      "Epochs[21]: train batch avg accuracy=0.21167968213558197, val accuracy=0.21281249821186066\n",
      "Epochs[22]: train batch avg accuracy=0.212890625, val accuracy=0.21015624701976776\n",
      "Epochs[23]: train batch avg accuracy=0.2169531285762787, val accuracy=0.21437500417232513\n",
      "Epochs[24]: train batch avg accuracy=0.2130468785762787, val accuracy=0.2123437523841858\n",
      "Epochs[25]: train batch avg accuracy=0.21203124523162842, val accuracy=0.2146874964237213\n",
      "Epochs[26]: train batch avg accuracy=0.21152344346046448, val accuracy=0.2107812464237213\n",
      "Epochs[27]: train batch avg accuracy=0.21027344465255737, val accuracy=0.21562500298023224\n",
      "Epochs[28]: train batch avg accuracy=0.21617187559604645, val accuracy=0.21406249701976776\n",
      "Epochs[29]: train batch avg accuracy=0.21593749523162842, val accuracy=0.2228125035762787\n",
      "Epochs[30]: train batch avg accuracy=0.21640625596046448, val accuracy=0.21781249344348907\n",
      "Epochs[31]: train batch avg accuracy=0.21230468153953552, val accuracy=0.21484375\n",
      "Epochs[32]: train batch avg accuracy=0.21070311963558197, val accuracy=0.2095312476158142\n",
      "Epochs[33]: train batch avg accuracy=0.2150000035762787, val accuracy=0.21828125417232513\n",
      "Epochs[34]: train batch avg accuracy=0.2091406285762787, val accuracy=0.21593749523162842\n",
      "Epochs[35]: train batch avg accuracy=0.21410156786441803, val accuracy=0.2214062511920929\n",
      "Epochs[36]: train batch avg accuracy=0.21382813155651093, val accuracy=0.2107812464237213\n",
      "Epochs[37]: train batch avg accuracy=0.21523436903953552, val accuracy=0.22046874463558197\n",
      "Epochs[38]: train batch avg accuracy=0.2135937511920929, val accuracy=0.21312500536441803\n",
      "Epochs[39]: train batch avg accuracy=0.21671874821186066, val accuracy=0.22093750536441803\n",
      "Epochs[40]: train batch avg accuracy=0.21156249940395355, val accuracy=0.21937499940395355\n",
      "Epochs[41]: train batch avg accuracy=0.21585936844348907, val accuracy=0.21921874582767487\n",
      "Epochs[42]: train batch avg accuracy=0.21390624344348907, val accuracy=0.21406249701976776\n",
      "Epochs[43]: train batch avg accuracy=0.21531249582767487, val accuracy=0.21484375\n",
      "Epochs[44]: train batch avg accuracy=0.21460936963558197, val accuracy=0.21765625476837158\n",
      "Epochs[45]: train batch avg accuracy=0.21035155653953552, val accuracy=0.21843749284744263\n",
      "Epochs[46]: train batch avg accuracy=0.21238280832767487, val accuracy=0.21812500059604645\n",
      "Epochs[47]: train batch avg accuracy=0.21398437023162842, val accuracy=0.21484375\n",
      "Epochs[48]: train batch avg accuracy=0.21238280832767487, val accuracy=0.22218750417232513\n",
      "Epochs[49]: train batch avg accuracy=0.21472656726837158, val accuracy=0.21484375\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"./tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "\n",
    "for e in range(iterations):\n",
    "    for X_batch, y_batch in zip(X_train_batches, y_train_batches):\n",
    "        sess.run(\n",
    "            optimizer,\n",
    "            feed_dict={\n",
    "                word_ids: X_batch,\n",
    "                y: y_batch\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if e % 5 == 0:\n",
    "        summary = sess.run(\n",
    "            merged,\n",
    "            feed_dict={\n",
    "                word_ids: X_val,\n",
    "                y: y_val\n",
    "            }\n",
    "        )\n",
    "    if e % 1 == 0:\n",
    "        train_acc = []\n",
    "        for X_batch, y_batch in zip(X_train_batches, y_train_batches):\n",
    "            train_acc.append(accuracy.eval(\n",
    "                feed_dict={word_ids: X_batch, y: y_batch}\n",
    "            ))\n",
    "        avg_tarin_acc = np.mean(train_acc)\n",
    "        val_acc = accuracy.eval(feed_dict={word_ids: X_val, y: y_val})\n",
    "        print(\n",
    "            f\"Epochs[{e}]: train batch avg accuracy={avg_tarin_acc}, val accuracy={val_acc}\")\n",
    "    writer.add_summary(summary, e)\n",
    "    writer.close()\n",
    "f = lambda src: pred_idx.eval(feed_dict={word_ids: src})\n",
    "# train_pred = f(X_train)\n",
    "test_pred = f(X_test)\n",
    "val_pred = f(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2ints(sentence):\n",
    "    tokens = sentence.split()\n",
    "    ids = [word_to_index[word] for word in tokens]\n",
    "    ids = pad_sequences([ids], maxlen=max_seq_length, padding=\"post\", truncating=\"post\")\n",
    "    return np.array(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
