{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import _pickle as pickle\n",
    "from typing import List\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json_rec\n",
    "from data_import import load_embedding_from_disks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/text_emotion.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./saved_model\"):\n",
    "    os.mkdir(\"./saved_model\")\n",
    "now = datetime.datetime.now()\n",
    "model_dir = \"./saved_model/\" + now.strftime(\"%Y-%m-%d-%H:%M\")\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "    os.mkdir(model_dir + \"/tensorboard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_writer = json_rec.ParamWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use default param? >>> Y\n"
     ]
    }
   ],
   "source": [
    "# Parameter\n",
    "# GLOVE_FILENAME = \"../data/glove.840B.300d.txt\"\n",
    "use_default = bool(input(\"Use default param? >>> \").upper() == \"Y\")\n",
    "if use_default:\n",
    "    param = dict(\n",
    "        GLOVE_FILENAME=\"../data/glove.6B.50d.txt\",\n",
    "        lstm_units=(512, 1024),\n",
    "        num_classes=13,\n",
    "        epochs=10,\n",
    "        max_seq_length=40\n",
    "    )\n",
    "else:\n",
    "    config_file = input(\"Directory of config file >>> \")\n",
    "    param_writer.read(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_rec.write_param(param, \"./sample_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10155it [00:00, 50912.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding from disks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:07, 51161.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding loaded from disks.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading embedding from disks...\")\n",
    "word_to_index, index_to_embedding = load_embedding_from_disks(\n",
    "    param[\"GLOVE_FILENAME\"],\n",
    "    with_indexes=True\n",
    ")\n",
    "print(\"Embedding loaded from disks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix loaded, Vocab Size: 400001\n",
      "Embedding Dim: 50\n"
     ]
    }
   ],
   "source": [
    "vocab_size, embedding_dim = index_to_embedding.shape\n",
    "print(f\"Embedding matrix loaded, Vocab Size: {vocab_size}\\nEmbedding Dim: {embedding_dim}\")\n",
    "SENTI_LST = list(set(df[\"sentiment\"]))\n",
    "globals().update(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2int(w: str) -> int:\n",
    "    try:\n",
    "        idx = word_to_index[w]\n",
    "    except KeyError:\n",
    "        idx = word_to_index[\"unk\"]\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 40)\n",
      "(40000, 13)\n"
     ]
    }
   ],
   "source": [
    "X_lst, y_lst = [], []\n",
    "for sentence, senti in zip(df[\"content\"], df[\"sentiment\"]):\n",
    "    # ==== Encode x ====\n",
    "    tokens = sentence.lower().split()\n",
    "    word_ints = np.array([word2int(x) for x in tokens])\n",
    "    X_lst.append(word_ints)\n",
    "    \n",
    "    # ==== Encode y ====\n",
    "    label = np.zeros([num_classes])\n",
    "    senti_index = SENTI_LST.index(senti)\n",
    "    label[senti_index] = 1\n",
    "    y_lst.append(label)\n",
    "    \n",
    "X_lst = pad_sequences(\n",
    "    X_lst,\n",
    "    maxlen=max_seq_length,\n",
    "    padding=\"post\",\n",
    "    truncating=\"post\"\n",
    ")\n",
    "\n",
    "X_raw = np.stack(X_lst)\n",
    "y_raw = np.stack(y_lst)\n",
    "print(X_raw.shape)\n",
    "print(y_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test,\n",
    " y_train, y_test) = train_test_split(\n",
    "    X_raw, y_raw,\n",
    "    test_size=0.2,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "(X_train, X_val,\n",
    " y_train, y_val) = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size=0.2,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing set generated,\n",
      "X_train shape: (25600, 40)\n",
      "y_train shape: (25600, 13)\n",
      "X_test shape: (8000, 40)\n",
      "y_test shape: (8000, 13)\n",
      "X_validation shape: (6400, 40)\n",
      "y_validation shape: (6400, 13)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training and testing set generated,\\\n",
    "\\nX_train shape: {X_train.shape}\\\n",
    "\\ny_train shape: {y_train.shape}\\\n",
    "\\nX_test shape: {X_test.shape}\\\n",
    "\\ny_test shape: {y_test.shape}\\\n",
    "\\nX_validation shape: {X_val.shape}\\\n",
    "\\ny_validation shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_batches = X_train.reshape(25, 1024, max_seq_length)\n",
    "y_train_batches = y_train.reshape(25, 1024, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session already cleaned.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sess.close()\n",
    "except NameError:\n",
    "    print(\"Session already cleaned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "with tf.name_scope(\"DATA_IO\"):\n",
    "    word_ids = tf.placeholder(\n",
    "        tf.int32,\n",
    "        shape=[None, max_seq_length]\n",
    "    )\n",
    "    \n",
    "    y = tf.placeholder(\n",
    "        tf.float32,\n",
    "        shape=[None, num_classes]\n",
    "    )\n",
    "\n",
    "with tf.name_scope(\"EMBEDDING\"):\n",
    "    embedding = tf.Variable(\n",
    "        tf.constant(0.0, shape=index_to_embedding.shape),\n",
    "        trainable=False,\n",
    "        name=\"EMBEDDING\"\n",
    "    )\n",
    "    \n",
    "    word_representation_layer = tf.nn.embedding_lookup(\n",
    "        params=embedding,\n",
    "        ids=word_ids\n",
    "    )\n",
    "    \n",
    "    embedding_placeholder = tf.placeholder(\n",
    "        tf.float32,\n",
    "        shape=index_to_embedding.shape\n",
    "    )\n",
    "    \n",
    "    embedding_init = embedding.assign(embedding_placeholder)\n",
    "    \n",
    "    _ = sess.run(\n",
    "        embedding_init, \n",
    "            feed_dict={\n",
    "                embedding_placeholder: index_to_embedding\n",
    "        }\n",
    "    )\n",
    "\n",
    "cell = tf.nn.rnn_cell.MultiRNNCell(\n",
    "    [tf.nn.rnn_cell.LSTMCell(\n",
    "        num_units=units,\n",
    "        name=f\"LSTM_LAYER_{i}\")\n",
    "        for i, units in enumerate(lstm_units)\n",
    "     ])\n",
    "\n",
    "lstm_cell = tf.contrib.rnn.DropoutWrapper(\n",
    "    cell=cell,\n",
    "    output_keep_prob=0.75\n",
    ")\n",
    "outputs, state = tf.nn.dynamic_rnn(\n",
    "    lstm_cell, \n",
    "    word_representation_layer,\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "with tf.name_scope(\"OUTPUT\"):\n",
    "    weight = tf.Variable(\n",
    "        tf.truncated_normal(\n",
    "            [lstm_units[-1], num_classes]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    bias = tf.Variable(\n",
    "        tf.random_normal(shape=[num_classes])\n",
    "    )\n",
    "\n",
    "# Option i)\n",
    "    value = tf.transpose(outputs, [1, 0, 2])\n",
    "    last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "# Option ii)\n",
    "#     last = outputs[:, -1, :]\n",
    "    pred = tf.matmul(last, weight) + bias\n",
    "    pred_idx = tf.argmax(pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Accuracy:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.name_scope(\"METRICS\"):\n",
    "    correct_pred = tf.equal(\n",
    "        tf.argmax(pred, axis=1),\n",
    "        tf.argmax(y, axis=1)\n",
    "    )\n",
    "\n",
    "    accuracy = tf.reduce_mean(\n",
    "        tf.cast(correct_pred, tf.float32)\n",
    "    )\n",
    "\n",
    "with tf.name_scope(\"LOSSES\"):\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=pred,\n",
    "            labels=y\n",
    "        )\n",
    "    )\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "tf.summary.scalar(\"Loss\", loss)\n",
    "tf.summary.scalar(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs[0]: train batch avg accuracy=0.21257811784744263, val accuracy=0.21453124284744263\n",
      "Epochs[1]: train batch avg accuracy=0.21019531786441803, val accuracy=0.21906250715255737\n",
      "Epochs[2]: train batch avg accuracy=0.21074219048023224, val accuracy=0.21390624344348907\n",
      "Epochs[3]: train batch avg accuracy=0.20976562798023224, val accuracy=0.21406249701976776\n",
      "Epochs[4]: train batch avg accuracy=0.21320313215255737, val accuracy=0.21375000476837158\n",
      "Epochs[5]: train batch avg accuracy=0.21277344226837158, val accuracy=0.21296875178813934\n",
      "Epochs[6]: train batch avg accuracy=0.21269531548023224, val accuracy=0.22374999523162842\n",
      "Epochs[7]: train batch avg accuracy=0.21125000715255737, val accuracy=0.2135937511920929\n",
      "Epochs[8]: train batch avg accuracy=0.20972655713558197, val accuracy=0.21968750655651093\n",
      "Epochs[9]: train batch avg accuracy=0.21160155534744263, val accuracy=0.21578125655651093\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = model_dir + \"/tensorboard/\"\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "\n",
    "for e in range(epochs):\n",
    "    for X_batch, y_batch in zip(X_train_batches, y_train_batches):\n",
    "        sess.run(\n",
    "            optimizer,\n",
    "            feed_dict={\n",
    "                word_ids: X_batch,\n",
    "                y: y_batch\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if e % 5 == 0:\n",
    "        summary = sess.run(\n",
    "            merged,\n",
    "            feed_dict={\n",
    "                word_ids: X_val,\n",
    "                y: y_val\n",
    "            }\n",
    "        )\n",
    "    if e % 1 == 0:\n",
    "        train_acc = []\n",
    "        for X_batch, y_batch in zip(X_train_batches, y_train_batches):\n",
    "            train_acc.append(accuracy.eval(\n",
    "                feed_dict={word_ids: X_batch, y: y_batch}\n",
    "            ))\n",
    "        avg_tarin_acc = np.mean(train_acc)\n",
    "        val_acc = accuracy.eval(feed_dict={word_ids: X_val, y: y_val})\n",
    "        print(\n",
    "            f\"Epochs[{e}]: train batch avg accuracy={avg_tarin_acc}, val accuracy={val_acc}\")\n",
    "    writer.add_summary(summary, e)\n",
    "    writer.close()\n",
    "f = lambda src: pred_idx.eval(feed_dict={word_ids: src})\n",
    "# train_pred = f(X_train)\n",
    "test_pred = f(X_test)\n",
    "val_pred = f(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ./saved_model/2019-01-20-01:38/model/saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.simple_save(\n",
    "            sess,\n",
    "            export_dir=f\"{model_dir}/model\",\n",
    "            inputs={\"word_ids\": word_ids, \"y\": y},\n",
    "            outputs={\"pred\": pred, \"pred_idx\": pred_idx})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2ints(sentence):\n",
    "    tokens = sentence.split()\n",
    "    ids = [word_to_index[word] for word in tokens]\n",
    "    ids = pad_sequences([ids], maxlen=max_seq_length, padding=\"post\", truncating=\"post\")\n",
    "    return np.array(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
